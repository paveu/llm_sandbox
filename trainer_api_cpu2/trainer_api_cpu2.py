import numpy as np
import evaluate
import torch
import torch.nn.functional as F  # Potrzebne do funkcji Softmax (zamiana surowych wynikÃ³w na %)
from datasets import load_dataset
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    TrainingArguments,
    Trainer,
    DataCollatorWithPadding,
)

# ==============================================================================
# 1. PRZYGOTOWANIE DANYCH I POJÄ˜Ä† (FILOZOFIA PRE-TRAININGU)
# ==============================================================================
checkpoint = "bert-base-uncased"
print(f"\n[1/6] Pobieranie modelu i danych: {checkpoint}...")

# DATASET: ZbiÃ³r par zdaÅ„. LABEL (Etykieta) to wynik: 1 (parafraza), 0 (rÃ³Å¼ne).
# Oryginalny BERT od Google uczyÅ‚ siÄ™ na 3.3 mld sÅ‚Ã³w (Wikipedia + KsiÄ…Å¼ki).
# On juÅ¼ "rozumie" jÄ™zyk, ale nie wie jeszcze, co to jest zadanie "MRPC".
raw_datasets = load_dataset("glue", "mrpc")

# TOKENIZER: Pobiera "SÅ‚ownik" (Vocab) przypisany do modelu.
# Zamienia sÅ‚owa na numery ID. SkÄ…d wie jakie numery? KaÅ¼dy model ma swÃ³j
# unikalny plik .txt ze sÅ‚ownikiem. Tutaj tekst staje siÄ™ matematykÄ….
tokenizer = AutoTokenizer.from_pretrained(checkpoint)


def tokenize_function(example):
    # ATTENTION HEADS (GÅ‚owy Uwagi): WewnÄ…trz modelu jest 12 warstw, a kaÅ¼da ma 12 gÅ‚Ã³w.
    # Razem 144 "mikro-mÃ³zgi", ktÃ³re analizujÄ… tekst pod rÃ³Å¼nymi kÄ…tami.
    # ÅÄ…czymy dwa zdania. Tokenizer doda [CLS] na poczÄ…tku i [SEP] miÄ™dzy zdaniami.
    # Truncation=True obcina zbyt dÅ‚ugie zdania do limitu modelu (np. 512 tokenÃ³w).
    return tokenizer(example["sentence1"], example["sentence2"], truncation=True)


print("\n[2/6] Tokenizacja (zamiana sÅ‚Ã³w na numery ID)...")
tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)

# EPOKA (Epoch): Przeczytanie caÅ‚ej "ksiÄ…Å¼ki" (zbioru danych) jeden raz.
# Epoch 1.0 oznacza, Å¼e model przejrzaÅ‚ wszystkie przykÅ‚ady dokÅ‚adnie raz.

# ZbiÃ³r TRENINGOWY (train): To nasze "zadania domowe". Tu model siÄ™ uczy.
# Wybieramy 200 przykÅ‚adÃ³w do nauki. shuffle(seed=42) miesza dane tak samo za kaÅ¼dym razem.
tokenized_datasets["train"] = tokenized_datasets["train"].shuffle(seed=42).select(range(200))

# ZbiÃ³r WALIDACYJNY (validation): To nasza "prÃ³bna matura".
# Model nie uczy siÄ™ na tych danych â€“ sprawdzamy tu, czy model faktycznie rozumie,
# czy tylko wykuÅ‚ przykÅ‚ady na pamiÄ™Ä‡ (tzw. overfitting).
tokenized_datasets["validation"] = tokenized_datasets["validation"].select(range(50))

# DATA COLLATOR: WyrÃ³wnuje dÅ‚ugoÅ›Ä‡ zdaÅ„ w paczce (batchu) dodajÄ…c zera (padding).
# Modele wymagajÄ…, aby dane w jednej paczce (batch) miaÅ‚y identyczny wymiar.
data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

# ==============================================================================
# 2. MODEL I WAGI (DODAWANIE NOWEJ GÅOWICY)
# ==============================================================================
print("\n[3/6] Åadowanie modelu i instalacja nowej 'GÅ‚owicy' klasyfikatora...")
# KLUCZOWY MOMENT: Odcinamy oryginalnÄ… gÅ‚owÄ™ BERT-a (do przewidywania sÅ‚Ã³w)
# i "przyszywamy" nowÄ…, klasyfikacyjnÄ… gÅ‚owÄ™ z 2 wyjÅ›ciami (TAK/NIE).
model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)

# WAGI (Weights): To miliony "pokrÄ™teÅ‚" (liczb) wewnÄ…trz modelu. Trening to krÄ™cenie nimi.
# KaÅ¼da waga decyduje, jak mocno dany sygnaÅ‚ wpÅ‚ywa na wynik koÅ„cowy.
# Wagi w "mÃ³zgu" sÄ… ustawione przez Google, ale w nowej gÅ‚owie sÄ… na razie LOSOWE.
weights_before = model.classifier.weight.data[0][:5].clone()
print(f"ğŸ‘‰ Wagi nowej gÅ‚owy PRZED treningiem (losowe): {weights_before}")

# ==============================================================================
# 3. TEST MODELU PRZED TRENINGIEM (Zgadujemy!)
# ==============================================================================
print("\n[3.5] TEST PRZED TRENINGIEM (Logity i Softmax):")
# Wyzwanie dla synonimÃ³w: czy model "poczuje" podobieÅ„stwo bez nauki?
z1 = "Pawel is here"
z2 = "Pawel is present"

# Przygotowanie danych do rÄ™cznego testu matematycznego
inputs = tokenizer(z1, z2, return_tensors="pt")

# torch.inference_mode() â€“ Jeszcze szybszy tryb "tylko do odczytu" niÅ¼ no_grad.
# CaÅ‚kowicie izoluje model od mechanizmÃ³w treningowych dla maksymalnej wydajnoÅ›ci CPU.

with torch.inference_mode():
    outputs_pre = model(**inputs)
    # LOGITY: Surowe punkty z modelu (np. [-1.2, 0.5]). Nie sumujÄ… siÄ™ do 100%.
    logits_pre = outputs_pre.logits

# SOFTMAX: Funkcja, ktÃ³ra zamienia surowe punkty (logity) na procenty (0-100%).
probs_pre = F.softmax(logits_pre, dim=-1)
print(f"ğŸ‘‰ Zdanie A: {z1} | Zdanie B: {z2}")
print(f"ğŸ‘‰ Surowe logity PRZED naukÄ…: {logits_pre}")
print(f"ğŸ‘‰ PewnoÅ›Ä‡ PRZED naukÄ… (Softmax): {probs_pre[0][1].item():.2%}")


# ==============================================================================
# 4. METRYKI, GRADIENTY I LOSS (Zasady oceniania)
# ==============================================================================
def compute_metrics(eval_preds):
    metric = evaluate.load("glue", "mrpc")
    logits, labels = eval_preds

    # LOSS (Strata): Matematyczna miara bÅ‚Ä™du. JeÅ›li spada, model lepiej rozumie dane.
    # GRADIENT: Instrukcja, w ktÃ³rÄ… stronÄ™ krÄ™ciÄ‡ wagÄ…, aby LOSS malaÅ‚.
    # GRAD_NORM: SiÅ‚a tej instrukcji (im wiÄ™kszy, tym gwaÅ‚towniejsza zmiana wag).

    # PREDICTIONS: To ostateczny "strzaÅ‚" modelu (odpowiedÅº na egzaminie).
    # Wybieramy indeks (0 lub 1), ktÃ³ry otrzymaÅ‚ najwiÄ™cej punktÃ³w w logitach.
    predictions = np.argmax(logits, axis=-1)

    # LABELS: To "klucz odpowiedzi" (prawdziwe etykiety ze zbioru danych).
    # Nauczyciel (metric) porÃ³wnuje predictions z labels.
    return metric.compute(predictions=predictions, references=labels)


# ==============================================================================
# 5. KONFIGURACJA TRENINGU (Zoptymalizowana pod Intel Ultra 7)
# ==============================================================================
training_args = TrainingArguments(
    output_dir="./test-trainer-cpu",
    # UÅ¼ywamy CPU, bo GPU zawiesza laptopa przy obliczeniach AI.
    use_cpu=True,
    eval_strategy="epoch",  # Sprawdzian (eval) po kaÅ¼dej peÅ‚nej epoce.
    num_train_epochs=3,  # Model przeczyta 200 zdaÅ„ 3 razy (lepsza stabilnoÅ›Ä‡).
    learning_rate=2e-5,  # "DÅ‚ugoÅ›Ä‡ kroku" (jak mocno gradient zmienia wagi).
    per_device_train_batch_size=4,  # Wykorzystujemy 14 rdzeni Twojego procesora.
    weight_decay=0.01,  # "Hamulec": zapobiega przypisywaniu ogromnych wag sÅ‚owom.
    logging_steps=5,  # Co 5 paczek wypisz stan w konsoli.
)

# ==============================================================================
# 6. TWORZENIE TRAINERA (DYRYGENT PROCESU)
# ==============================================================================
# Trainer Å‚Ä…czy model, dane, parametry i metryki w jednÄ… maszynÄ™ treningowÄ….
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation"],
    data_collator=data_collator,
    processing_class=tokenizer,
    compute_metrics=compute_metrics,
)

# ==============================================================================
# 7. TRENING I ANALIZA ZMIAN W "MÃ“ZGU"
# ==============================================================================
print("\n[4/6] Start Fine-tuningu (Trening nowej gÅ‚owy na Intel Ultra 7)...")
# LOSS (Strata) powinna spadaÄ‡ z kaÅ¼dym krokiem.
trainer.train()

print("\n[5/6] Sprawdzanie zmian w 'mÃ³zgu' modelu...")
weights_after = model.classifier.weight.data[0][:5].clone()
print(f"ğŸ‘‰ Wagi przed: {weights_before}")
print(f"ğŸ‘‰ Wagi po:    {weights_after}")

# RÃ“Å»NICA: Pokazuje o ile fizycznie przesunÄ™Å‚y siÄ™ wagi pod wpÅ‚ywem uczenia.
diff = weights_after - weights_before
print(f"ğŸ‘‰ RÃ³Å¼nica (fizyczny efekt nauki): {diff}")

# ==============================================================================
# 8. TEST PRAKTYCZNY PO TRENINGU (SYNONYM TEST)
# ==============================================================================
print("\n[6/6] TEST PO TRENINGU (Analiza synonimÃ³w):")
# Ponownie uÅ¼ywamy inference_mode dla najszybszego sprawdzenia wyniku.
with torch.inference_mode():
    outputs_post = model(**inputs)
    # Ponownie zamieniamy logity na % po treningu
    probs_post = F.softmax(outputs_post.logits, dim=-1)
    confidence = probs_post[0][1].item()

print(f"ğŸ‘‰ Zdanie A: {z1} | Zdanie B: {z2}")
print(f"ğŸ‘‰ Wynik PRZED naukÄ…: {probs_pre[0][1].item():.2%}")
print(f"ğŸ‘‰ Wynik PO nauce:    {confidence:.2%}")

# --- ANALIZA TECHNICZNA DLA KOLEGI (WHY THIS MATTERS) ---
# 1. SEMANTIC SIMILARITY: Model musiaÅ‚ wykazaÄ‡ siÄ™ wiedzÄ… z pre-trainingu,
#    Å¼eby wiedzieÄ‡, Å¼e "here" i "present" to w tym kontekÅ›cie synonimy.
#
# 2. ROLE OF ATTENTION: Twoje 144 gÅ‚owy uwagi analizujÄ… kontekst sÅ‚owa "Pawel".
#    W obu zdaniach Pawel peÅ‚ni tÄ™ samÄ… rolÄ™ (podmiot), co pomaga modelowi.
#
# 3. CPU EFFICIENCY: Trening trwaÅ‚ krÃ³tko, bo TwÃ³j Ultra 7 Å›wietnie
#    radzi sobie z matematykÄ… macierzowÄ… dziÄ™ki instrukcjom AVX/AMX.

# ==============================================================================
# 9. ZAPISYWANIE MODELU NA DYSKU
# ==============================================================================
# Zapisujemy wagi modelu i sÅ‚ownik tokenizera do folderu.
trainer.save_model("./moj_model_synonimy")
tokenizer.save_pretrained("./moj_model_synonimy")
print("\nModel zapisany w './moj_model_synonimy'!")