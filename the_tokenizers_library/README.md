
#  Hugging Face Tokenizers Mastery

Ten projekt to kompletne kompendium wiedzy o potokach tokenizacji (Tokenization Pipelines) w ekosystemie  Transformers. Zawiera on implementacje r贸偶nych algorytm贸w, techniki trenowania od zera oraz praktyczne zastosowania w zadaniach NLP.

##  Struktura Repozytorium

### 1. Specjalistyczny Trening (Training)
* **`01_training_a_specialized_code_tokenizer.py`**: Trenowanie tokenizera BPE na korpusie Python (CodeParrot). Implementuje wykorzystanie **generator贸w (yield)**, co zapobiega przepenieniu pamici RAM przy du偶ych zbiorach danych.

### 2. Zaawansowane Funkcje (Fast Tokenizer Features)
* **`02_fast_tokenizers_special_powers.py`**: Wykorzystanie mapowania przesuni (**offset_mapping**) w zadaniu NER. Obsuguje logik grupowania token贸w typu subword przy u偶yciu etykiet B- i I-.
* **`03_fast_tokenizers_in_the_qa_pipeline.py`**: Implementacja strategii **Sliding Window** (przesuwne okno) z parametrem `stride`. Pozwala na ekstrakcj odpowiedzi z kontekst贸w przekraczajcych limit 512 token贸w.

### 3. Komponenty Potoku (Pipeline Components)
* **`04.normalization_and_pretokenization.py`**: Analiza r贸偶nic w podejciu do tekstu midzy modelami BERT (Whitespace + Punctuation) a GPT-2 (Byte-level).
* **`05_byte_pair_encoding_tokenization.py`**: Demonstracja etap贸w normalizacji (NFD, StripAccents) oraz pre-tokenizacji.

### 4. Gbia Algorytm贸w (Algorithm Deep Dives)
* **`05_bpe_logic.py`**: Implementacja czenia najczstszych par token贸w.
* **`06_wordpiece_tokenization.py`**: Symulacja algorytmu WordPiece z wykorzystaniem wzoru: $score = \frac{freq\_pary}{freq\_el1 \times freq\_el2}$.
* **`07_unigram_tokenization.py`**: Model probabilistyczny wybierajcy segmentacj o najni偶szej stracie (loss).

### 5. Budowanie Blokowe (Block-by-Block)
Zbi贸r skrypt贸w pokazujcy, jak zo偶y kompletny tokenizer z poszczeg贸lnych "klock贸w" biblioteki  Tokenizers:
* **`08_1_wordpiece_bert.py`**: Odtworzenie potoku BERT (Lowercase -> Whitespace -> WordPiece).
* **`08_2_bpe_gpt2.py`**: Bezzwrotna rekonstrukcja tekstu (lossless) przy u偶yciu Byte-level BPE.
* **`08_3_building_unigram_xlnet.py`**: Implementacja normalizacji SentencePiece i pre-tokenizacji Metaspace.

##  Kluczowe Koncepcje Wykorzystane w Kodzie

| Koncepcja | Opis |
| :--- | :--- |
| **Normalization** | Wstpne czyszczenie tekstu (NFD, NFKD, Lowercase) przed podziaem. |
| **Pre-tokenization** | Podzia na sowa, zachowujcy informacj o spacjach (np. znak `` lub `_`). |
| **Post-processing** | Automatyczne dodawanie token贸w specjalnych jak `[CLS]` i `[SEP]`. |
| **Offset Mapping** | Powizanie token贸w z ich pozycj w oryginalnym tekcie znakowym. |

---
*Projekt zrealizowany w oparciu o Rozdzia 6 kursu Hugging Face NLP.*