import torch
from torch.utils.data import DataLoader
from torch.optim import AdamW
import torch.nn.functional as F
from datasets import load_dataset
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    get_scheduler,
    DataCollatorWithPadding
)
from accelerate import Accelerator
from tqdm.auto import tqdm
import evaluate

# ==============================================================================
# 1. INICJALIZACJA I PRZYGOTOWANIE DANYCH
# ==============================================================================
checkpoint = "bert-base-uncased"
print(f"\n[1/7] KROK 1: Inicjalizacja komponentÃ³w...")
print(f"ğŸ‘‰ Åadowanie modelu bazowego: {checkpoint}")

# Accelerator: Automatycznie zarzÄ…dza sprzÄ™tem (CPU/GPU/TPU).
# Na Twoim Intel Ultra 7 przypisze obliczenia do procesora.
# To serce biblioteki ğŸ¤— Accelerate, ktÃ³re dba o wydajnoÅ›Ä‡ na Twoim sprzÄ™cie.
accelerator = Accelerator()
device = accelerator.device
print(f"ğŸ‘‰ Aktywne urzÄ…dzenie (Device): {device}")

# Åadowanie danych MRPC (czy zdania sÄ… parafrazami)
# Dataset: ZbiÃ³r par zdaÅ„. LABEL (Etykieta) to wynik: 1 (parafraza), 0 (rÃ³Å¼ne).
raw_datasets = load_dataset("glue", "mrpc")
tokenizer = AutoTokenizer.from_pretrained(checkpoint)

def tokenize_function(example):
    # Funkcja mapujÄ…ca: zamieniamy tekst na liczby zrozumiaÅ‚e dla BERT-a.
    return tokenizer(example["sentence1"], example["sentence2"], truncation=True)

print("ğŸ‘‰ Rozpoczynam tokenizacjÄ™ (zamiana tekstu na wektory liczbowe)...")
tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)

# CZYSZCZENIE DANYCH: PyTorch akceptuje tylko liczby. Usuwamy tekst, zostawiamy tensory.
# W czystym PyTorch (w przeciwieÅ„stwie do Trainera) musimy to zrobiÄ‡ rÄ™cznie,
# inaczej model "pogubi siÄ™" prÃ³bujÄ…c przetwarzaÄ‡ napisy.
print("ğŸ‘‰ Czyszczenie kolumn i ustawianie formatu tensora...")
tokenized_datasets = tokenized_datasets.remove_columns(["sentence1", "sentence2", "idx"])
tokenized_datasets = tokenized_datasets.rename_column("label", "labels")
tokenized_datasets.set_format("torch")

# Wybieramy maÅ‚e prÃ³bki do testu na CPU (dla szybkoÅ›ci treningu na laptopie)
train_dataset = tokenized_datasets["train"].shuffle(seed=42).select(range(200))
eval_dataset = tokenized_datasets["validation"].select(range(50))
print(f"ğŸ‘‰ Gotowe! Rozmiar zbioru treningowego: {len(train_dataset)}, walidacyjnego: {len(eval_dataset)}")

# ==============================================================================
# 2. DATALOADERY (POMPY DANYCH - SZCZEGÃ“ÅOWE WYJAÅšNIENIE)
# ==============================================================================
# DataCollator: Odpowiada za dynamiczne wyrÃ³wnywanie dÅ‚ugoÅ›ci zdaÅ„ w paczkach.
data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

# train_dataloader: To "taÅ›mociÄ…g", ktÃ³ry dostarcza dane do modelu podczas nauki.
# - shuffle=True: Bardzo waÅ¼ne! Miesza kolejnoÅ›Ä‡ przykÅ‚adÃ³w w kaÅ¼dej epoce.
#   DziÄ™ki temu model nie uczy siÄ™ kolejnoÅ›ci pytaÅ„, tylko zasad jÄ™zyka.
# - batch_size=4: Model nie czyta 200 zdaÅ„ naraz. Czyta je "kÄ™sami" po 4 sztuki.
#   To pozwala oszczÄ™dziÄ‡ pamiÄ™Ä‡ RAM Twojego komputera.
# WYJAÅšNIENIE: DataLoader zamienia TwÃ³j zestaw danych w iterator, ktÃ³ry
# automatycznie tworzy paczki (batches) i nakÅ‚ada na nie padding.
print("\n[2/7] KROK 2: Przygotowanie DataloaderÃ³w (taÅ›mociÄ…gÃ³w danych)...")
train_dataloader = DataLoader(
    train_dataset,
    shuffle=True,
    batch_size=4,
    collate_fn=data_collator
)

# eval_dataloader: TaÅ›mociÄ…g dla danych testowych (egzaminacyjnych).
# - Tutaj NIE mieszamy danych (shuffle=False domyÅ›lnie), bo kolejnoÅ›Ä‡ przy
#   sprawdzaniu wynikÃ³w nie wpÅ‚ywa na proces nauki, a uÅ‚atwia analizÄ™ bÅ‚Ä™dÃ³w.
eval_dataloader = DataLoader(
    eval_dataset,
    batch_size=4,
    collate_fn=data_collator
)

# ==============================================================================
# 3. MODEL I TEST PRZED NAUKÄ„ (ANALIZA MATEMATYCZNA)
# ==============================================================================
print("\n[3/7] KROK 3: Åadowanie modelu i analiza przed treningiem...")
model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)
# AdamW: Optymalizator z poprawkÄ… na zanikanie wag (weight decay).
optimizer = AdamW(model.parameters(), lr=5e-5)

z1, z2 = "Pawel is here", "Pawel is present"
# Zamieniamy nasze zdania testowe na format modelu i wysyÅ‚amy na CPU/GPU.
inputs = tokenizer(z1, z2, return_tensors="pt").to(device)

# --- WYJAÅšNIENIE BLOKU LOGITÃ“W I SOFTMAXU ---
# with torch.no_grad(): - WyÅ‚Ä…czamy "tryb nagrywania" gradientÃ³w.
# Przy samej predykcji (zgadywaniu) model nie musi pamiÄ™taÄ‡ Å›cieÅ¼ki obliczeÅ„.
# To drastycznie przyspiesza dziaÅ‚anie i zuÅ¼ywa mniej pamiÄ™ci.
with torch.no_grad():
    # model(**inputs) - Przekazujemy dane przez sieÄ‡ neuronowÄ….
    # .logits - Model zwraca surowe wyniki (punkty) dla kaÅ¼dej klasy (0 i 1).
    # Te liczby mogÄ… byÄ‡ dowolne, np. [-2.1, 1.5]. Trudno je zrozumieÄ‡ czÅ‚owiekowi.
    # LOGITY to surowy output ostatniej warstwy liniowej przed jakÄ…kolwiek normalizacjÄ….
    logits_pre = model(**inputs).logits

    # F.softmax(logits_pre, dim=-1) - Magiczna funkcja matematyczna.
    # Bierze surowe logity (np. -2.1 i 1.5) i zamienia je na prawdopodobieÅ„stwo (0% - 100%).
    # Po Softmaxie suma wynikÃ³w dla obu klas zawsze wynosi dokÅ‚adnie 1 (czyli 100%).
    # dim=-1 oznacza, Å¼e liczymy to dla ostatniego wymiaru (czyli dla naszych klas).
    # SOFTMAX pozwala nam zinterpretowaÄ‡ wynik jako "pewnoÅ›Ä‡ modelu".
    probs_pre = F.softmax(logits_pre, dim=-1)

print(f"ğŸ‘‰ Zdanie A: {z1} | Zdanie B: {z2}")
print(f"ğŸ‘‰ PewnoÅ›Ä‡ przed naukÄ… (Softmax): {probs_pre[0][1].item():.2%}")

# ==============================================================================
# 4. KONFIGURACJA ACCELERATE I HARMONOGRAMU (SCHEDULER)
# ==============================================================================
print("\n[4/7] KROK 4: Konfiguracja Accelerate i Schedulera...")

# prepare(): To tutaj Accelerate przejmuje kontrolÄ™ nad obiektami.
# Dataloadery zostanÄ… zoptymalizowane pod kÄ…tem Twojego procesora.
train_dataloader, eval_dataloader, model, optimizer = accelerator.prepare(
    train_dataloader, eval_dataloader, model, optimizer
)

num_epochs = 3
num_training_steps = num_epochs * len(train_dataloader)

# --- WYJAÅšNIENIE LR_SCHEDULER (HARMONOGRAMU UCZENIA) ---
# lr_scheduler: Kontroluje "wspÃ³Å‚czynnik uczenia" (Learning Rate).
# - "linear": Oznacza, Å¼e zaczynamy od peÅ‚nej prÄ™dkoÅ›ci (5e-5), a z kaÅ¼dym krokiem
#   treningu model uczy siÄ™ coraz wolniej i ostroÅ¼niej (aÅ¼ do zera).
# - optimizer: Musi wiedzieÄ‡, czyim "tempem" steruje (optymalizator trzyma wagi).
# - num_warmup_steps=0: Okres rozgrzewki. Gdyby wynosiÅ‚ np. 100, model zaczÄ…Å‚by
#   bardzo powoli i przyspieszaÅ‚ przez pierwsze 100 krokÃ³w. Tu startujemy od razu.
# - num_training_steps: Harmonogram musi wiedzieÄ‡, jak dÅ‚ugo trwa caÅ‚y trening,
#   aby mÃ³c idealnie rozÅ‚oÅ¼yÄ‡ spadek prÄ™dkoÅ›ci (tzw. decay) w czasie.
# WYJAÅšNIENIE: Scheduler zapobiega "przestrzeleniu" celu (overshooting) pod koniec treningu.
lr_scheduler = get_scheduler(
    "linear",
    optimizer=optimizer,
    num_warmup_steps=0,
    num_training_steps=num_training_steps,
)
# ==============================================================================
# 5. PÄ˜TLA TRENINGOWA
# ==============================================================================

print(f"\n[5/7] KROK 5: Rozpoczynam pÄ™tlÄ™ treningowÄ… (Manual Training Loop)...")
progress_bar = tqdm(range(num_training_steps))

model.train()  # Aktywujemy tryb treningowy (waÅ¼ne dla warstw takich jak Dropout i BatchNorm)
for epoch in range(num_epochs):
    print(f"\n--- Epoka {epoch + 1} / {num_epochs} ---")
    for step, batch in enumerate(train_dataloader):
        # Forward pass: Model przewiduje wyniki dla aktualnej paczki (batch).
        outputs = model(**batch)

        # Loss: Obliczamy matematycznÄ… karÄ™ za bÅ‚Ä™dy modelu.
        loss = outputs.loss

        # Backward pass: Obliczamy gradienty (pochodne bÅ‚Ä™du).
        # accelerator.backward() zastÄ™puje standardowe loss.backward() w PyTorch.
        accelerator.backward(loss)

        # Aktualizacja wag: Poprawiamy "pokrÄ™tÅ‚a" modelu na podstawie obliczonych gradientÃ³w.
        optimizer.step()

        # Aktualizacja tempa nauki: Scheduler obniÅ¼a learning rate zgodnie z planem liniowym.
        lr_scheduler.step()

        # Wyzerowanie gradientÃ³w: CzyÅ›cimy pamiÄ™Ä‡ bÅ‚Ä™du przed kolejnÄ… paczkÄ….
        # W PyTorch gradienty siÄ™ sumujÄ… (akumulujÄ…), wiÄ™c musimy je rÄ™cznie czyÅ›ciÄ‡!
        optimizer.zero_grad()

        progress_bar.update(1)

# ==============================================================================
# 6. EWALUACJA (SPRAWDZIAN KOÅƒCOWY)
# ==============================================================================
print("\n[6/7] KROK 6: Rozpoczynam sprawdzian modelu (Ewaluacja)...")
metric = evaluate.load("glue", "mrpc")
model.eval()  # WyÅ‚Ä…czamy funkcje treningowe. Model ma teraz tylko stabilnie odpowiadaÄ‡.

for batch in eval_dataloader:
    # Podczas ewaluacji nigdy nie liczymy gradientÃ³w (oszczÄ™dnoÅ›Ä‡ czasu i energii CPU).
    with torch.no_grad():
        outputs = model(**batch)

    logits = outputs.logits
    # argmax: Wybieramy indeks (0 lub 1), ktÃ³ry dostaÅ‚ najwiÄ™cej punktÃ³w (najwyÅ¼szy logit).
    predictions = torch.argmax(logits, dim=-1)

    # Przekazujemy wyniki paczki do globalnego licznika metryk.
    metric.add_batch(predictions=predictions, references=batch["labels"])

print(f"ğŸ‘‰ WYNIKI KOÅƒCOWE METRYKI: {metric.compute()}")

# ==============================================================================
# 7. TEST PO NAUCE (PORÃ“WNANIE I PRZENOSZENIE DANYCH)
# ==============================================================================
print("\n[7/7] KROK 7: KoÅ„cowy test praktyczny i zapisywanie modelu...")

with torch.no_grad():
    # --- WYJAÅšNIENIE PRZENOSZENIA DANYCH (TO DEVICE) ---
    # inputs = {k: v.to(device) for k, v in inputs.items()}
    # To jest krytyczne! W PyTorch model i dane MUSZÄ„ byÄ‡ na tym samym "urzÄ…dzeniu".
    # JeÅ›li model jest na GPU, a dane na CPU (lub odwrotnie) - program siÄ™ zawiesi.
    # Ta linia bierze nasz sÅ‚ownik 'inputs' (tekst zamieniony na liczby) i upewnia siÄ™,
    # Å¼e kaÅ¼da jego czÄ™Å›Ä‡ (input_ids, attention_mask) jest tam, gdzie nasz model.
    # WYJAÅšNIENIE: PoniewaÅ¼ nasz model przeszedÅ‚ przez accelerator.prepare(),
    # moÅ¼e znajdowaÄ‡ siÄ™ na specyficznym urzÄ…dzeniu. Dane testowe muszÄ… tam "doÅ‚Ä…czyÄ‡".
    inputs = {k: v.to(device) for k, v in inputs.items()}

    # Ponownie przepuszczamy te same zdania o Pawle przez model, ktÃ³ry juÅ¼ siÄ™ czegoÅ› nauczyÅ‚.
    # .logits pobiera surowe wyniki z modelu po procesie fine-tuningu.
    logits_post = model(**inputs).logits

    # Ponownie zamieniamy logity (surowe punkty) na procenty (Softmax).
    # Teraz sprawdzimy, czy model jest bardziej pewny, Å¼e "here" i "present" to to samo.
    # F.softmax wykonuje operacjÄ™: e^xi / suma(e^xj).
    probs_post = F.softmax(logits_post, dim=-1)

print("\n--- ANALIZA PORÃ“WNAWCZA (TEST SYNONIMÃ“W) ---")
print(f"ğŸ‘‰ Zdanie A: {z1} | Zdanie B: {z2}")
print(f"ğŸ‘‰ PewnoÅ›Ä‡ podobieÅ„stwa PRZED naukÄ…: {probs_pre[0][1].item():.2%}")
print(f"ğŸ‘‰ PewnoÅ›Ä‡ podobieÅ„stwa PO nauce:    {probs_post[0][1].item():.2%}")

# Zapisujemy efekt naszej pracy:
# unwrap_model: WyciÄ…ga czysty model PyTorch z "opakowania" Accelerate.
# Jest to niezbÄ™dne, aby zapisaÄ‡ pliki w standardowym formacie Transformers.
unwrapped_model = accelerator.unwrap_model(model)
path = "./pytorch_model_custom"
unwrapped_model.save_pretrained(path)
tokenizer.save_pretrained(path)
print(f"\nâœ… Trening zakoÅ„czony! Model zapisany w folderze: {path}")