import torch
from torch.utils.data import DataLoader
from torch.optim import AdamW
import torch.nn.functional as F
from datasets import load_dataset
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    get_scheduler,
    DataCollatorWithPadding
)
from accelerate import Accelerator
from tqdm.auto import tqdm
import evaluate

# ==============================================================================
# 1. INICJALIZACJA I PRZYGOTOWANIE DANYCH
# ==============================================================================
checkpoint = "bert-base-uncased"
print(f"\n[1/7] Inicjalizacja komponent贸w dla: {checkpoint}...")

# Accelerator: Automatycznie zarzdza sprztem (CPU/GPU/TPU).
# Na Twoim Intel Ultra 7 przypisze obliczenia do procesora.
accelerator = Accelerator()
device = accelerator.device

# adowanie danych MRPC (czy zdania s parafrazami)
raw_datasets = load_dataset("glue", "mrpc")
tokenizer = AutoTokenizer.from_pretrained(checkpoint)

def tokenize_function(example):
    return tokenizer(example["sentence1"], example["sentence2"], truncation=True)

tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)

# CZYSZCZENIE DANYCH: PyTorch akceptuje tylko liczby. Usuwamy tekst, zostawiamy tensory.
tokenized_datasets = tokenized_datasets.remove_columns(["sentence1", "sentence2", "idx"])
tokenized_datasets = tokenized_datasets.rename_column("label", "labels")
tokenized_datasets.set_format("torch")

# Wybieramy mae pr贸bki do testu na CPU
train_dataset = tokenized_datasets["train"].shuffle(seed=42).select(range(200))
eval_dataset = tokenized_datasets["validation"].select(range(50))

# ==============================================================================
# 2. DATALOADERY (POMPY DANYCH - SZCZEGOWE WYJANIENIE)
# ==============================================================================
data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

# train_dataloader: To "tamocig", kt贸ry dostarcza dane do modelu podczas nauki.
# - shuffle=True: Bardzo wa偶ne! Miesza kolejno przykad贸w w ka偶dej epoce.
#   Dziki temu model nie uczy si kolejnoci pyta, tylko zasad jzyka.
# - batch_size=4: Model nie czyta 200 zda naraz. Czyta je "ksami" po 4 sztuki.
#   To pozwala oszczdzi pami RAM Twojego komputera.
train_dataloader = DataLoader(
    train_dataset,
    shuffle=True,
    batch_size=4,
    collate_fn=data_collator
)

# eval_dataloader: Tamocig dla danych testowych (egzaminacyjnych).
# - Tutaj NIE mieszamy danych (shuffle=False domylnie), bo kolejno przy
#   sprawdzaniu wynik贸w nie wpywa na proces nauki, a uatwia analiz bd贸w.
eval_dataloader = DataLoader(
    eval_dataset,
    batch_size=4,
    collate_fn=data_collator
)

# ==============================================================================
# 3. MODEL I TEST PRZED NAUK (ANALIZA MATEMATYCZNA)
# ==============================================================================
model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)
optimizer = AdamW(model.parameters(), lr=5e-5)

z1, z2 = "Pawel is here", "Pawel is present"
inputs = tokenizer(z1, z2, return_tensors="pt").to(device)

# --- WYJANIENIE BLOKU LOGITW I SOFTMAXU ---
# with torch.no_grad(): - Wyczamy "tryb nagrywania" gradient贸w.
# Przy samej predykcji (zgadywaniu) model nie musi pamita cie偶ki oblicze.
# To drastycznie przyspiesza dziaanie i zu偶ywa mniej pamici.
with torch.no_grad():
    # model(**inputs) - Przekazujemy dane przez sie neuronow.
    # .logits - Model zwraca surowe wyniki (punkty) dla ka偶dej klasy (0 i 1).
    # Te liczby mog by dowolne, np. [-2.1, 1.5]. Trudno je zrozumie czowiekowi.
    logits_pre = model(**inputs).logits

    # F.softmax(logits_pre, dim=-1) - Magiczna funkcja matematyczna.
    # Bierze surowe logity (np. -2.1 i 1.5) i zamienia je na prawdopodobiestwo (0% - 100%).
    # Po Softmaxie suma wynik贸w dla obu klas zawsze wynosi dokadnie 1 (czyli 100%).
    # dim=-1 oznacza, 偶e liczymy to dla ostatniego wymiaru (czyli dla naszych klas).
    probs_pre = F.softmax(logits_pre, dim=-1)

# ==============================================================================
# 4. KONFIGURACJA ACCELERATE I SCHEDULERA (HARMONOGRAMU)
# ==============================================================================
train_dataloader, eval_dataloader, model, optimizer = accelerator.prepare(
    train_dataloader, eval_dataloader, model, optimizer
)

num_epochs = 3
num_training_steps = num_epochs * len(train_dataloader)

# --- WYJANIENIE LR_SCHEDULER (HARMONOGRAMU UCZENIA) ---
# lr_scheduler: Kontroluje "wsp贸czynnik uczenia" (Learning Rate).
# - "linear": Oznacza, 偶e zaczynamy od penej prdkoci (5e-5), a z ka偶dym krokiem
#   treningu model uczy si coraz wolniej i ostro偶niej (a偶 do zera).
# - optimizer: Musi wiedzie, czyim "tempem" steruje.
# - num_warmup_steps=0: Okres rozgrzewki. Gdyby wynosi np. 100, model zaczby
#   bardzo powoli i przyspiesza przez pierwsze 100 krok贸w. Tu startujemy od razu.
# - num_training_steps: Harmonogram musi wiedzie, jak dugo trwa cay trening,
#   aby m贸c idealnie rozo偶y spadek prdkoci w czasie.
lr_scheduler = get_scheduler(
    "linear",
    optimizer=optimizer,
    num_warmup_steps=0,
    num_training_steps=num_training_steps,
)
# ==============================================================================
# 5. PTLA TRENINGOWA
# ==============================================================================

print(f"\n[2/7] Start treningu PyTorch na {device}...")
progress_bar = tqdm(range(num_training_steps))

model.train()  # Aktywujemy tryb treningowy (wa偶ne dla warstw takich jak Dropout)
for epoch in range(num_epochs):
    for batch in train_dataloader:
        outputs = model(**batch)
        loss = outputs.loss
        accelerator.backward(loss)  # Obliczamy gradienty (pochodne bdu)

        optimizer.step()  # Aktualizujemy wagi modelu na podstawie gradient贸w
        lr_scheduler.step()  # Informujemy harmonogram, 偶e wykonalimy krok (zmniejsz LR)
        optimizer.zero_grad()  # Czycimy "pami bdu" przed kolejn paczk danych
        progress_bar.update(1)

# ==============================================================================
# 6. EWALUACJA (SPRAWDZIAN KOCOWY)
# ==============================================================================
metric = evaluate.load("glue", "mrpc")
model.eval()  # Wyczamy funkcje treningowe. Model ma teraz tylko stabilnie odpowiada.

for batch in eval_dataloader:
    with torch.no_grad():
        outputs = model(**batch)

    logits = outputs.logits
    predictions = torch.argmax(logits, dim=-1)
    metric.add_batch(predictions=predictions, references=batch["labels"])

print(f" Wyniki kocowe: {metric.compute()}")

# ==============================================================================
# 7. TEST PO NAUCE (PORWNANIE I PRZENOSZENIE DANYCH)
# ==============================================================================
with torch.no_grad():
    # --- WYJANIENIE PRZENOSZENIA DANYCH (TO DEVICE) ---
    # inputs = {k: v.to(device) for k, v in inputs.items()}
    # To jest krytyczne! W PyTorch model i dane MUSZ by na tym samym "urzdzeniu".
    # Jeli model jest na GPU, a dane na CPU (lub odwrotnie) - program si zawiesi.
    # Ta linia bierze nasz sownik 'inputs' (tekst zamieniony na liczby) i upewnia si,
    # 偶e ka偶da jego cz (input_ids, attention_mask) jest tam, gdzie nasz model.
    inputs = {k: v.to(device) for k, v in inputs.items()}

    # Ponownie przepuszczamy te same zdania o Pawle przez model, kt贸ry ju偶 si czego nauczy.
    logits_post = model(**inputs).logits

    # Ponownie zamieniamy logity (surowe punkty) na procenty (Softmax).
    # Teraz sprawdzimy, czy model jest bardziej pewny, 偶e "here" i "present" to to samo.
    probs_post = F.softmax(logits_post, dim=-1)

print("\n--- PORWNANIE SYNONYM TEST ---")
print(f" Zdanie A: {z1} | Zdanie B: {z2}")
print(f" Pewno podobiestwa PRZED nauk: {probs_pre[0][1].item():.2%}")
print(f" Pewno podobiestwa PO nauce:    {probs_post[0][1].item():.2%}")

# Zapisujemy efekt naszej pracy
unwrapped_model = accelerator.unwrap_model(model)
unwrapped_model.save_pretrained("./a_full_training_loop_with_pytorch/pytorch_model_custom")
tokenizer.save_pretrained("./a_full_training_loop_with_pytorch/pytorch_model_custom")
print("\n[7/7] Trening zakoczony pomylnie!")