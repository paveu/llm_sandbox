import torch
from torch.utils.data import DataLoader
from torch.optim import AdamW
import torch.nn.functional as F
from datasets import load_dataset
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    get_scheduler,
    DataCollatorWithPadding
)
from accelerate import Accelerator
from tqdm.auto import tqdm
import evaluate

# ==============================================================================
# 1. INICJALIZACJA I PRZYGOTOWANIE DANYCH
# ==============================================================================
checkpoint = "bert-base-uncased"
print(f"\n[1/7] KROK 1: Inicjalizacja komponentÃ³w...")
print(f"ğŸ‘‰ Åadowanie modelu bazowego: {checkpoint}")

# Accelerator: Automatycznie zarzÄ…dza sprzÄ™tem (CPU/GPU/TPU).
# Na Twoim Intel Ultra 7 przypisze obliczenia do procesora.
# To serce biblioteki ğŸ¤— Accelerate, ktÃ³re dba o wydajnoÅ›Ä‡ na Twoim sprzÄ™cie.
# GÅ‚Ã³wnym zadaniem Accelerate jest umoÅ¼liwienie treningu rozproszonego na wielu GPU/TPU przy minimalnych zmianach w kodzie.
accelerator = Accelerator()
device = accelerator.device
print(f"ğŸ‘‰ Aktywne urzÄ…dzenie (Device): {device}")

# Åadowanie danych MRPC (czy zdania sÄ… parafrazami)
# Dataset: ZbiÃ³r par zdaÅ„. LABEL (Etykieta) to wynik: 1 (parafraza), 0 (rÃ³Å¼ne).
# Drugi argument w load_dataset (np. 'mrpc') okreÅ›la konkretne zadanie lub podzbiÃ³r (subset) w ramach danego benchmarku GLUE.
raw_datasets = load_dataset("glue", "mrpc")
tokenizer = AutoTokenizer.from_pretrained(checkpoint)

def tokenize_function(example):
    # Funkcja mapujÄ…ca: zamieniamy tekst na liczby zrozumiaÅ‚e dla BERT-a.
    # UÅ¼ycie batched=True w metodzie .map() przetwarza wiele przykÅ‚adÃ³w naraz, co radykalnie przyspiesza proces tokenizacji.
    return tokenizer(example["sentence1"], example["sentence2"], truncation=True)

print("ğŸ‘‰ Rozpoczynam tokenizacjÄ™ (zamiana tekstu na wektory liczbowe)...")
tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)

# CZYSZCZENIE DANYCH: PyTorch akceptuje tylko liczby. Usuwamy tekst, zostawiamy tensory.
# W czystym PyTorch (w przeciwieÅ„stwie do Trainera) musimy to zrobiÄ‡ rÄ™cznie,
# inaczej model "pogubi siÄ™" prÃ³bujÄ…c przetwarzaÄ‡ napisy.
# Usuwamy kolumny surowego tekstu, bo model oczekuje tensorÃ³w liczbowych; prÃ³ba ich zachowania mogÅ‚aby spowodowaÄ‡ bÅ‚Ä™dy.
print("ğŸ‘‰ Czyszczenie kolumn i ustawianie formatu tensora...")
tokenized_datasets = tokenized_datasets.remove_columns(["sentence1", "sentence2", "idx"])
tokenized_datasets = tokenized_datasets.rename_column("label", "labels")

# --- KLUCZOWY MOMENT KONWERSJI ---
# DomyÅ›lnie 'datasets' zwraca listy Pythona. Model BERT (PyTorch) wymaga jednak
# obiektÃ³w typu torch.Tensor do obliczeÅ„ macierzowych. PoniÅ¼sza linia
# automatycznie "opakowuje" dane w tensory, co pozwala na ich bezpoÅ›rednie
# przesyÅ‚anie do modelu i na kartÄ™ graficznÄ…/procesor.
tokenized_datasets.set_format("torch")

# Wybieramy maÅ‚e prÃ³bki do testu na CPU (dla szybkoÅ›ci treningu na laptopie)
train_dataset = tokenized_datasets["train"].shuffle(seed=42).select(range(200))
eval_dataset = tokenized_datasets["validation"].select(range(50))
print(f"ğŸ‘‰ Gotowe! Rozmiar zbioru treningowego: {len(train_dataset)}, walidacyjnego: {len(eval_dataset)}")

# ==============================================================================
# 2. DATALOADERY (POMPY DANYCH - SZCZEGÃ“ÅOWE WYJAÅšNIENIE)
# ==============================================================================
# DataCollator: Odpowiada za dynamiczne wyrÃ³wnywanie dÅ‚ugoÅ›ci zdaÅ„ w paczkach.
# Dynamiczne dopeÅ‚nianie (Dynamic Padding) jest wydajniejsze niÅ¼ staÅ‚e, bo ogranicza rozmiar do najdÅ‚uÅ¼szego zdania TYLKO w danej partii (batch).
data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

# train_dataloader: To "taÅ›mociÄ…g", ktÃ³ry dostarcza dane do modelu podczas nauki.
# - shuffle=True: Bardzo waÅ¼ne! Miesza kolejnoÅ›Ä‡ przykÅ‚adÃ³w w kaÅ¼dej epoce.
#   DziÄ™ki temu model nie uczy siÄ™ kolejnoÅ›ci pytaÅ„, tylko zasad jÄ™zyka.
# - batch_size=4: Model nie czyta 200 zdaÅ„ naraz. Czyta je "kÄ™sami" po 4 sztuki.
#   To pozwala oszczÄ™dziÄ‡ pamiÄ™Ä‡ RAM Twojego komputera.
# WYJAÅšNIENIE: DataLoader zamienia TwÃ³j zestaw danych w iterator, ktÃ³ry
# automatycznie tworzy paczki (batches) i nakÅ‚ada na nie padding.
print("\n[2/7] KROK 2: Przygotowanie DataloaderÃ³w (taÅ›mociÄ…gÃ³w danych)...")
train_dataloader = DataLoader(
    train_dataset,
    shuffle=True,
    batch_size=4,
    collate_fn=data_collator
)

# eval_dataloader: TaÅ›mociÄ…g dla danych testowych (egzaminacyjnych).
# - Tutaj NIE mieszamy danych (shuffle=False domyÅ›lnie), bo kolejnoÅ›Ä‡ przy
#   sprawdzaniu wynikÃ³w nie wpÅ‚ywa na proces nauki, a uÅ‚atwia analizÄ™ bÅ‚Ä™dÃ³w.
eval_dataloader = DataLoader(
    eval_dataset,
    batch_size=4,
    collate_fn=data_collator
)

# ==============================================================================
# 3. MODEL I TEST PRZED NAUKÄ„ (ANALIZA MATEMATYCZNA)
# ==============================================================================
print("\n[3/7] KROK 3: Åadowanie modelu i analiza przed treningiem...")
# --- ÅADOWANIE ARCHITEKTURY I WAG ---
# AutoModelForSequenceClassification: Pobiera architekturÄ™ BERT-a i automatycznie
# dodaje na jej szczycie "gÅ‚owicÄ™ klasyfikacyjnÄ…" (warstwÄ™ Linear).
# - checkpoint: Wczytuje wyuczone juÅ¼ wagi jÄ™zyka (wiedza o gramatyce i znaczeniu sÅ‚Ã³w).
# - num_labels=2: MÃ³wi modelowi, Å¼e na koÅ„cu ma mieÄ‡ 2 wyjÅ›cia (w tym przypadku:
#   0 - to nie parafraza, 1 - to parafraza).
model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)

# --- SILNIK UCZENIA (OPTYMALIZATOR) ---
# AdamW: Wyrafinowana wersja algorytmu spadku gradientu. To on decyduje,
# jak mocno zmieniÄ‡ "pokrÄ™tÅ‚a" (parametry) modelu, aby zmniejszyÄ‡ bÅ‚Ä…d (loss).
# - model.parameters(): Dajemy optymalizatorowi dostÄ™p do wszystkich wag modelu,
#   ktÃ³re ma prawo modyfikowaÄ‡.
# - lr=5e-5: (Learning Rate) "PrÄ™dkoÅ›Ä‡ nauki". Bardzo maÅ‚a wartoÅ›Ä‡ (0.00005),
#   aby model nie "zapomniaÅ‚" tego, co juÅ¼ wie, a jedynie delikatnie dostosowaÅ‚ siÄ™
#   do nowego zadania (fine-tuning).
# - 'W' w AdamW (Weight Decay): Technika zapobiegajÄ…ca przeuczeniu â€“ model
#   stara siÄ™ trzymaÄ‡ wagi na niskim poziomie, co promuje prostsze rozwiÄ…zania.
# KluczowÄ… rÃ³Å¼nicÄ… miÄ™dzy Adam a AdamW jest to, Å¼e AdamW stosuje odizolowanÄ… (decoupled) regularyzacjÄ™ spadku wag.
optimizer = AdamW(model.parameters(), lr=5e-5)

z1, z2 = "Pawel is here", "Pawel is present"
# Zamieniamy nasze zdania testowe na format modelu i wysyÅ‚amy na CPU/GPU.
# Przenosimy dane na device (np. GPU), poniewaÅ¼ model i dane muszÄ… znajdowaÄ‡ siÄ™ na tym samym urzÄ…dzeniu dla obliczeÅ„.
inputs = tokenizer(z1, z2, return_tensors="pt").to(device)

# --- WYJAÅšNIENIE TRYBU INFERENCJI I INTERPRETACJI WYNIKÃ“W ---
# with torch.inference_mode(): - Nowsza, bezpieczniejsza i szybsza wersja no_grad().
# CaÅ‚kowicie izoluje model od mechanizmu gradientÃ³w. W Å›wiecie LLM "inferencja"
# to moment, w ktÃ³rym model nie uczy siÄ™, a jedynie wykorzystuje zdobytÄ… wiedzÄ™.
with torch.inference_mode():
    # model(**inputs) - Przekazujemy dane przez sieÄ‡ neuronowÄ….
    # .logits - Model zwraca surowe wyniki (punkty) dla kaÅ¼dej klasy (0 i 1).
    # KONTEKST LLM: Sieci neuronowe na ostatniej warstwie nie "myÅ›lÄ…" kategoriami
    # prawdy czy faÅ‚szu, ale "napiÄ™ciem" na neuronach wyjÅ›ciowych.
    # LOGITY to wÅ‚aÅ›nie te surowe wartoÅ›ci â€“ im wyÅ¼szy logit, tym bardziej model
    # "wierzy" w danÄ… klasÄ™, ale te liczby sÄ… nienormalizowane (np. mogÄ… wynosiÄ‡ 5.4 i -1.2).
    # Pole 'token_type_ids' w BERT informuje model, ktÃ³ry token naleÅ¼y do ktÃ³rej sekwencji w parze zdaÅ„.
    logits_pre = model(**inputs).logits

    # F.softmax(logits_pre, dim=-1) - Magiczna funkcja matematyczna.
    # KONTEKST LLM: PoniewaÅ¼ trudno operowaÄ‡ na logitach, uÅ¼ywamy Softmaxu, aby:
    # 1. SprowadziÄ‡ wszystkie wyniki do przedziaÅ‚u (0, 1) â€“ czyli prawdopodobieÅ„stwa.
    # 2. SprawiÄ‡, by suma wszystkich wynikÃ³w wynosiÅ‚a 1.0 (100%).
    # To kluczowy moment: dziÄ™ki temu wiemy, czy model jest "pewny na 99%", czy "waha siÄ™ 51/49".
    # dim=-1 oznacza, Å¼e liczymy to dla ostatniego wymiaru (czyli dla naszych klas).
    probs_pre = F.softmax(logits_pre, dim=-1)

print(f"ğŸ‘‰ Zdanie A: {z1} | Zdanie B: {z2}")
print(f"ğŸ‘‰ PewnoÅ›Ä‡ przed naukÄ… (Softmax): {probs_pre[0][1].item():.2%}")

# ==============================================================================
# 4. KONFIGURACJA ACCELERATE I HARMONOGRAMU (SCHEDULER)
# ==============================================================================
print("\n[4/7] KROK 4: Konfiguracja Accelerate i Schedulera...")
# ===========================================================================

# --- WYJAÅšNIENIE FUNKCJI PREPARE() ---
# accelerator.prepare(): To najwaÅ¼niejszy moment w pracy z bibliotekÄ… Accelerate.
# Ta linia "owija" (wrapuje) Twoje obiekty w inteligentne opakowania, ktÃ³re:
# 1. MODEL I OPTYMALIZATOR: Przenosi je na odpowiednie urzÄ…dzenie (CPU, GPU lub wiele GPU).
# 2. DATALOADERY: Zmienia je w wersje, ktÃ³re potrafiÄ… dostarczaÄ‡ dane do modelu
#    w sposÃ³b zsynchronizowany ze sprzÄ™tem.
# 3. AUTOMATYZACJA: DziÄ™ki temu ten sam kod uruchomisz na swoim laptopie z procesorem
#    Intel Ultra 7, jak i na potÄ™Å¼nym klastrze obliczeniowym, bez zmiany ani jednej linii kodu.
# WYJAÅšNIENIE: Zamiast rÄ™cznie pisaÄ‡ .to(device) dla kaÅ¼dego elementu,
# powierzasz to zadanie Acceleratorowi, ktÃ³ry dba o maksymalnÄ… wydajnoÅ›Ä‡.
# Przy uÅ¼yciu Accelerate, wÅ‚Ä…czenie 'fp16=True' w argumentach umoÅ¼liwiÅ‚oby trening z 16-bitowÄ… precyzjÄ…, oszczÄ™dzajÄ…c pamiÄ™Ä‡ i przyspieszajÄ…c naukÄ™.
train_dataloader, eval_dataloader, model, optimizer = accelerator.prepare(
    train_dataloader, eval_dataloader, model, optimizer
)

num_epochs = 3
num_training_steps = num_epochs * len(train_dataloader)

# --- WYJAÅšNIENIE LR_SCHEDULER (HARMONOGRAMU UCZENIA) ---
# lr_scheduler: Kontroluje "wspÃ³Å‚czynnik uczenia" (Learning Rate).
# - "linear": Oznacza, Å¼e zaczynamy od peÅ‚nej prÄ™dkoÅ›ci (5e-5), a z kaÅ¼dym krokiem
#   treningu model uczy siÄ™ coraz wolniej i ostroÅ¼niej (aÅ¼ do zera).
# - optimizer: Musi wiedzieÄ‡, czyim "tempem" steruje (optymalizator trzyma wagi).
# - num_warmup_steps=0: Okres rozgrzewki. Gdyby wynosiÅ‚ np. 100, model zaczÄ…Å‚by
#   bardzo powoli i przyspieszaÅ‚ przez pierwsze 100 krokÃ³w. Tu startujemy od razu.
# - num_training_steps: Harmonogram musi wiedzieÄ‡, jak dÅ‚ugo trwa caÅ‚y trening,
#   aby mÃ³c idealnie rozÅ‚oÅ¼yÄ‡ spadek prÄ™dkoÅ›ci (tzw. decay) w czasie.
# WYJAÅšNIENIE: Scheduler zapobiega "przestrzeleniu" celu (overshooting) pod koniec treningu.
# Parametr 'eval_strategy' (w klasie Trainer) okreÅ›laÅ‚by, czy ewaluacja odbywa siÄ™ co okreÅ›lonÄ… liczbÄ™ krokÃ³w ('steps'), czy co epokÄ™.
lr_scheduler = get_scheduler(
    "linear",
    optimizer=optimizer,
    num_warmup_steps=0,
    num_training_steps=num_training_steps,
)
# ==============================================================================
# 5. PÄ˜TLA TRENINGOWA (PROCES NAUKI)
# ==============================================================================

print(f"\n[5/7] KROK 5: Rozpoczynam pÄ™tlÄ™ treningowÄ… (Manual Training Loop)...")
progress_bar = tqdm(range(num_training_steps))

# model.train(): PrzeÅ‚Ä…cza model w tryb uczenia. NiektÃ³re warstwy (jak Dropout)
# zachowujÄ… siÄ™ inaczej podczas treningu niÅ¼ podczas testÃ³w. To sygnaÅ‚ dla modelu:
# "BÄ™dziemy aktualizowaÄ‡ TwojÄ… wiedzÄ™, bÄ…dÅº w gotowoÅ›ci".
model.train()

for epoch in range(num_epochs):
    print(f"\n--- Epoka {epoch + 1} / {num_epochs} ---")
    for step, batch in enumerate(train_dataloader):
        # --- KROK 1: FORWARD PASS (PRZEJÅšCIE DO PRZODU) ---
        # Logika kolejnoÅ›ci: Najpierw Forward, poniewaÅ¼ model musi najpierw "zgadnÄ…Ä‡" wynik, Å¼ebyÅ›my mogli sprawdziÄ‡, jak bardzo siÄ™ pomyliÅ‚ wzglÄ™dem prawdy.
        # Przepuszczamy dane przez wszystkie warstwy sieci. Model generuje
        # przewidywania (logits) i automatycznie porÃ³wnuje je z poprawnymi
        # odpowiedziami (labels) zawartymi w 'batch'.
        outputs = model(**batch)

        # --- KROK 2: LOSS CALCULATION (OBLICZANIE STRATY) ---
        # 'loss' to pojedyncza liczba mÃ³wiÄ…ca o tym, jak bardzo model siÄ™ pomyliÅ‚.
        # Im wiÄ™kszy bÅ‚Ä…d, tym wyÅ¼sza strata. Naszym celem jest zminimalizowanie tej liczby.
        loss = outputs.loss

        # --- KROK 3: BACKWARD PASS (PROPAGACJA WSTECZNA) ---
        # Logika kolejnoÅ›ci: Na podstawie bÅ‚Ä™du (loss) obliczamy gradienty (mapÄ™ poprawek). Bez bÅ‚Ä™du nie wiedzielibyÅ›my, co poprawiaÄ‡.
        # accelerator.backward(loss): Obliczamy tzw. gradienty dla kaÅ¼dego parametru.
        # Gradient to informacja: "O ile i w ktÃ³rÄ… stronÄ™ muszÄ™ przesunÄ…Ä‡ to konkretne
        # pokrÄ™tÅ‚o w modelu, Å¼eby strata (loss) byÅ‚a mniejsza?".
        # W bibliotece Accelerate zastÄ™pujemy standardowe loss.backward() metodÄ… accelerator.backward(loss).
        accelerator.backward(loss)

        # --- KROK 4: OPTIMIZER STEP (AKTUALIZACJA WAG) ---
        # Logika kolejnoÅ›ci: Dopiero teraz mamy "mapÄ™" zmian (gradienty) i moÅ¼emy faktycznie fizycznie zmieniÄ‡ wagi modelu (przekrÄ™ciÄ‡ pokrÄ™tÅ‚a).
        # Teraz, gdy wiemy juÅ¼, w ktÃ³rÄ… stronÄ™ krÄ™ciÄ‡ pokrÄ™tÅ‚ami (mamy gradienty),
        # optymalizator AdamW faktycznie wykonuje ten ruch, zmieniajÄ…c wagi modelu.
        optimizer.step()

        # --- KROK 5: SCHEDULER STEP (KOREKTA PRÄ˜DKOÅšCI) ---
        # Logika kolejnoÅ›ci: Skoro model wÅ‚aÅ›nie zrobiÅ‚ krok i siÄ™ czegoÅ› nauczyÅ‚, aktualizujemy LR, Å¼eby stawaÅ‚ siÄ™ coraz precyzyjniejszy w kolejnych krokach.
        # Zgodnie z planem liniowym, po kaÅ¼dej aktualizacji wag nieco zmniejszamy
        # wspÃ³Å‚czynnik uczenia (Learning Rate). Model z czasem staje siÄ™ coraz
        # bardziej "ostroÅ¼ny" w swoich zmianach.
        lr_scheduler.step()

        # --- KROK 6: ZERO GRAD (CZYSZCZENIE PAMIÄ˜CI) ---
        # Logika kolejnoÅ›ci: Na koÅ„cu usuwamy stare gradienty, bo juÅ¼ zostaÅ‚y zuÅ¼yte do poprawy wag. Musimy mieÄ‡ "czystÄ… kartÄ™" dla nastÄ™pnej paczki danych.
        # KLUCZOWE: PyTorch domyÅ›lnie dodaje nowe gradienty do starych.
        # JeÅ›li ich nie wyzerujemy, model "pogubi siÄ™", sumujÄ…c poprawki z poprzednich
        # paczek danych. CzyÅ›cimy tablicÄ™ przed kolejnym krokiem.
        optimizer.zero_grad()

        progress_bar.update(1)

# 'Gradient Accumulation' pozwala symulowaÄ‡ wiÄ™kszy batch size poprzez akumulacjÄ™ gradientÃ³w z kilku mniejszych krokÃ³w przed wykonaniem optimizer.step().

# ==============================================================================
# 6. EWALUACJA (EGZAMIN GENERALNY MODELU)
# ==============================================================================
print("\n[6/7] KROK 6: Rozpoczynam sprawdzian modelu (Ewaluacja)...")

# evaluate.load: Pobieramy gotowy "arkusz ocen" dla zadania MRPC.
# Metryki (takie jak Accuracy czy F1-score) pozwalajÄ… nam obiektywnie oceniÄ‡,
# czy model faktycznie rozumie jÄ™zyk, czy tylko zgaduje.
# Zadaniem funkcji compute_metrics (lub biblioteki evaluate) jest przekonwertowanie logitÃ³w na przewidywania i obliczenie miar jakoÅ›ci.
metric = evaluate.load("glue", "mrpc")

# model.eval(): PrzeÅ‚Ä…cza model w tryb "Egzaminu".
# Jest to absolutnie kluczowe! WyÅ‚Ä…cza mechanizmy takie jak Dropout, ktÃ³re
# podczas treningu celowo wprowadzajÄ… szum, by model byÅ‚ odporniejszy.
# W trybie eval() model staje siÄ™ stabilny i deterministyczny.
# model.eval() zmienia tryb pracy warstw (np. Dropout, Batchnorm) na odpowiedni dla fazy inferencji.
"""
WYJAÅšNIENIE FAZY INFERENCJI (WNIOSKOWANIA):
To moment, w ktÃ³rym model wykorzystuje zamroÅ¼onÄ… wiedzÄ™ do przewidywania wynikÃ³w, nie zmieniajÄ…c juÅ¼ swoich wag.
1. Brak nauki: Nie wykonujemy krokÃ³w backward() ani optimizer.step() - oszczÄ™dzamy czas i pamiÄ™Ä‡.
2. StabilnoÅ›Ä‡: Warstwy takie jak Dropout sÄ… wyÅ‚Ä…czone, aby kaÅ¼da predykcja byÅ‚a staÅ‚a i oparta na wszystkich neuronach.
3. Kierunek: Dane pÅ‚ynÄ… wyÅ‚Ä…cznie "do przodu" (Forward Pass) - od tekstu wejÅ›ciowego do logitÃ³w na wyjÅ›ciu.
To odpowiednik wykorzystania wiedzy w praktyce (np. przez uÅ¼ytkownika aplikacji) po zakoÅ„czeniu etapu nauki.
"""

for batch in eval_dataloader:
    # --- TRYB BEZ GRADIENTÃ“W (ZAMIAST no_grad MOÅ»NA UÅ»YÄ† inference_mode) ---
    # Podczas sprawdzianu nie chcemy zmieniaÄ‡ wag modelu ani traciÄ‡ pamiÄ™ci
    # na zapamiÄ™tywanie Å›cieÅ¼ki obliczeÅ„ do Backpropagation.
    # To sprawia, Å¼e proces jest duÅ¼o szybszy i zuÅ¼ywa uÅ‚amek pamiÄ™ci RAM.
    # UÅ¼ycie torch.no_grad() lub inference_mode podczas ewaluacji oszczÄ™dza pamiÄ™Ä‡ i przyspiesza obliczenia poprzez wyÅ‚Ä…czenie Å›ledzenia gradientÃ³w.
    with torch.inference_mode():
        outputs = model(**batch)

    # Logits: Pobieramy "pewnoÅ›Ä‡ siebie" modelu dla kaÅ¼dej z dwÃ³ch klas.
    logits = outputs.logits

    # --- KROK 1: ARGMAX (DECYZJA MODELU) ---
    # Model wyrzuca logity (np. [-1.2, 3.5]). Funkcja argmax patrzy, ktÃ³ra liczba
    # jest wiÄ™ksza i zwraca jej indeks (w tym przypadku '1').
    # To jest moment, w ktÃ³rym model finalnie mÃ³wi nam: "UwaÅ¼am, Å¼e to parafraza".
    predictions = torch.argmax(logits, dim=-1)

    # --- KROK 2: AKUMULACJA WYNIKÃ“W ---
    # metric.add_batch: Nie oceniamy modelu po jednej paczce.
    # Zbieramy wszystkie przewidywania (predictions) i porÃ³wnujemy je
    # z prawdziwymi odpowiedziami (references/labels).
    # Metryka gromadzi te dane w pamiÄ™ci, by na koÅ„cu obliczyÄ‡ Å›redniÄ….
    metric.add_batch(predictions=predictions, references=batch["labels"])

# metric.compute(): Finalne obliczenie wynikÃ³w (np. % poprawnych odpowiedzi).
# JeÅ›li do obiektu Trainer nie podano by 'eval_dataset', trening by trwaÅ‚, ale nie otrzymalibyÅ›my raportÃ³w o metrykach podczas nauki.
print(f"ğŸ‘‰ WYNIKI KOÅƒCOWE METRYKI: {metric.compute()}")

# ==============================================================================
# 7. TEST PRAKTYCZNY I ZAPISYWANIE (WERYFIKACJA EFEKTÃ“W)
# ==============================================================================
print("\n[7/7] KROK 7: KoÅ„cowy test praktyczny i zapisywanie modelu...")

# Zmieniamy na inference_mode() dla lepszej wydajnoÅ›ci i bezpieczeÅ„stwa.
with torch.inference_mode():
    # --- WYJAÅšNIENIE PRZENOSZENIA DANYCH (TO DEVICE) ---
    # Ta linia to "odprawa celna" dla danych. W PyTorch model i dane MUSZÄ„ przebywaÄ‡
    # w tej samej pamiÄ™ci (np. oba na CPU lub oba na GPU).
    # PoniewaÅ¼ accelerator.prepare() mÃ³gÅ‚ przenieÅ›Ä‡ model na konkretne urzÄ…dzenie,
    # musimy upewniÄ‡ siÄ™, Å¼e nasze nowe, testowe zdania teÅ¼ tam trafiÄ….
    inputs = {k: v.to(device) for k, v in inputs.items()}

    # --- PROCES PREDYKCJI (INFERENCJA) ---
    # Przepuszczamy zdania "Pawel is here" i "Pawel is present" przez sieÄ‡.
    # Model uÅ¼ywa teraz swoich zaktualizowanych wag (tych "pokrÄ™teÅ‚", ktÃ³re
    # ustawiliÅ›my w Kroku 5), aby oceniÄ‡ podobieÅ„stwo.
    logits_post = model(**inputs).logits

    # --- SOFTMAX (INTERPRETACJA DLA CZÅOWIEKA) ---
    # Softmax zamienia logity na prawdopodobieÅ„stwo.
    # Interesuje nas wartoÅ›Ä‡ pod indeksem [0][1], czyli "PrawdopodobieÅ„stwo, Å¼e to parafraza".
    # WartoÅ›Ä‡ 1.0 = 100% pewnoÅ›ci, 0.5 = model nie wie, 0.0 = na pewno nie parafraza.
    probs_post = F.softmax(logits_post, dim=-1)

print("\n--- ANALIZA PORÃ“WNAWCZA (TEST SYNONIMÃ“W) ---")
print(f"ğŸ‘‰ Zdanie A: {z1} | Zdanie B: {z2}")
print(f"ğŸ‘‰ PewnoÅ›Ä‡ podobieÅ„stwa PRZED naukÄ…: {probs_pre[0][1].item():.2%}")
print(f"ğŸ‘‰ PewnoÅ›Ä‡ podobieÅ„stwa PO nauce:    {probs_post[0][1].item():.2%}")

# Zapisujemy efekt naszej pracy:
# unwrap_model: WyciÄ…ga czysty model PyTorch z "opakowania" Accelerate.
# Jest to niezbÄ™dne, aby zapisaÄ‡ pliki w standardowym formacie Transformers.
unwrapped_model = accelerator.unwrap_model(model)
path = "./pytorch_model_custom"
unwrapped_model.save_pretrained(path)
tokenizer.save_pretrained(path)
print(f"\nâœ… Trening zakoÅ„czony! Model zapisany w folderze: {path}")